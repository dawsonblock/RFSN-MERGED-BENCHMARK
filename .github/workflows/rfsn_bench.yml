name: RFSN Benchmark

on:
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Dataset to run (swebench_lite, swebench_verified, swebench_full)'
        required: true
        default: 'swebench_lite'
        type: choice
        options:
          - swebench_lite
          - swebench_verified
          - swebench_full
      max_tasks:
        description: 'Maximum number of tasks to run (0 for all)'
        required: false
        default: '10'
        type: string
      strict_mode:
        description: 'Enforce strict mode (required for official benchmarks)'
        required: true
        default: true
        type: boolean
      parallel_tasks:
        description: 'Number of parallel tasks'
        required: false
        default: '1'
        type: string

env:
  RFSN_BENCH_STRICT: ${{ github.event.inputs.strict_mode && '1' || '0' }}
  PYTHONUNBUFFERED: '1'

jobs:
  verify-environment:
    name: Verify Environment
    runs-on: ubuntu-latest
    outputs:
      strict_mode: ${{ steps.check.outputs.strict }}
    steps:
      - name: Verify strict mode for benchmark
        id: check
        run: |
          echo "strict=${{ env.RFSN_BENCH_STRICT }}" >> $GITHUB_OUTPUT
          if [ "${{ env.RFSN_BENCH_STRICT }}" = "1" ]; then
            echo "✅ Strict mode enabled"
          else
            echo "⚠️ Warning: Strict mode disabled - results may not be valid for official benchmarks"
          fi

  benchmark:
    name: Run RFSN Benchmark
    needs: verify-environment
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e '.[dev,llm]'
          
      - name: Verify strict mode is set
        run: |
          python -c "
          import os
          strict = os.environ.get('RFSN_BENCH_STRICT', '0')
          if strict != '1':
              print('WARNING: Strict mode not enabled')
          else:
              print('✅ RFSN_BENCH_STRICT=1')
          "
          
      - name: Create run directories
        run: |
          mkdir -p runs
          mkdir -p eval_results
          
      - name: Run benchmark
        id: benchmark
        run: |
          MAX_TASKS="${{ github.event.inputs.max_tasks }}"
          if [ "$MAX_TASKS" = "0" ]; then
            MAX_TASKS_ARG=""
          else
            MAX_TASKS_ARG="--max-tasks $MAX_TASKS"
          fi
          
          python -m eval.cli \
            --dataset ${{ github.event.inputs.dataset }} \
            $MAX_TASKS_ARG \
            --parallel ${{ github.event.inputs.parallel_tasks }} \
            --output-dir runs/ \
            --results-dir eval_results/ \
            2>&1 | tee benchmark.log
            
      - name: Generate results summary
        if: always()
        run: |
          python -c "
          import json
          from pathlib import Path
          
          results_dir = Path('eval_results')
          results = []
          
          for f in results_dir.glob('*_results.jsonl'):
              with open(f) as fp:
                  for line in fp:
                      if line.strip():
                          results.append(json.loads(line))
          
          summary = {
              'total': len(results),
              'passed': sum(1 for r in results if r.get('success')),
              'failed': sum(1 for r in results if not r.get('success')),
              'by_status': {},
          }
          
          for r in results:
              status = r.get('status', 'UNKNOWN')
              summary['by_status'][status] = summary['by_status'].get(status, 0) + 1
          
          with open('results.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print('=== Benchmark Summary ===')
          print(f\"Total: {summary['total']}\")
          print(f\"Passed: {summary['passed']}\")
          print(f\"Failed: {summary['failed']}\")
          print(f\"Pass rate: {summary['passed']/max(1, summary['total'])*100:.1f}%\")
          print('Status breakdown:', summary['by_status'])
          "
          
      - name: Generate step summary
        if: always()
        run: |
          cat << 'EOF' >> $GITHUB_STEP_SUMMARY
          ## RFSN Benchmark Results
          
          | Setting | Value |
          |---------|-------|
          | Dataset | ${{ github.event.inputs.dataset }} |
          | Max Tasks | ${{ github.event.inputs.max_tasks }} |
          | Strict Mode | ${{ github.event.inputs.strict_mode }} |
          | Parallel Tasks | ${{ github.event.inputs.parallel_tasks }} |
          
          EOF
          
          if [ -f results.json ]; then
            python -c "
          import json
          with open('results.json') as f:
              data = json.load(f)
          print('### Summary')
          print(f\"- **Total Tasks**: {data['total']}\")
          print(f\"- **Passed**: {data['passed']}\")
          print(f\"- **Failed**: {data['failed']}\")
          print(f\"- **Pass Rate**: {data['passed']/max(1,data['total'])*100:.1f}%\")
          print()
          print('### Status Breakdown')
          for status, count in data.get('by_status', {}).items():
              print(f'- {status}: {count}')
          " >> $GITHUB_STEP_SUMMARY
          fi
          
      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            runs/*/controller.log
            runs/*/events.jsonl
            runs/*/patch.diff
            runs/*/evidence/
            eval_results/
            results.json
            benchmark.log
          retention-days: 30
          
      - name: Upload results.json
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-json
          path: results.json
          retention-days: 90
