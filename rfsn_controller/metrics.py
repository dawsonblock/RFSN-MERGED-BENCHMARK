"""Prometheus metrics for RFSN Controller.

Provides comprehensive metrics collection for monitoring performance,
success rates, and system health.

Usage:
    from rfsn_controller.metrics import (
        proposals_total,
        proposals_rejected,
        repair_duration,
        start_metrics_server
    )
    
    # Increment counters
    proposals_total.labels(intent='repair', action_type='edit_file').inc()
    
    # Time operations
    with repair_duration.time():
        result = controller.repair()
    
    # Start metrics endpoint
    start_metrics_server(port=9090)
"""

from __future__ import annotations

import time

from prometheus_client import (
    REGISTRY,
    Counter,
    Gauge,
    Histogram,
    Info,
    start_http_server,
)

# =============================================================================
# Proposal Metrics
# =============================================================================

proposals_total = Counter(
    'rfsn_proposals_total',
    'Total number of proposals generated by planner',
    ['intent', 'action_type'],
    registry=REGISTRY
)

proposals_rejected = Counter(
    'rfsn_proposals_rejected_total',
    'Number of proposals rejected by gate',
    ['rejection_type', 'intent'],
    registry=REGISTRY
)

proposals_accepted = Counter(
    'rfsn_proposals_accepted_total',
    'Number of proposals accepted by gate',
    ['intent', 'action_type'],
    registry=REGISTRY
)

# =============================================================================
# Repair Session Metrics
# =============================================================================

repair_duration = Histogram(
    'rfsn_repair_duration_seconds',
    'Time to complete a repair session',
    buckets=[1, 5, 10, 30, 60, 120, 300, 600, 1200, 1800, 3600],
    registry=REGISTRY
)

repair_success = Counter(
    'rfsn_repair_success_total',
    'Number of successful repairs',
    ['project_type'],
    registry=REGISTRY
)

repair_failure = Counter(
    'rfsn_repair_failure_total',
    'Number of failed repairs',
    ['failure_reason', 'project_type'],
    registry=REGISTRY
)

active_repairs = Gauge(
    'rfsn_active_repairs',
    'Number of currently active repair sessions',
    registry=REGISTRY
)

# =============================================================================
# Test Execution Metrics
# =============================================================================

tests_executed = Counter(
    'rfsn_tests_executed_total',
    'Total number of test executions',
    ['test_type'],  # 'focused', 'full', 'regression'
    registry=REGISTRY
)

tests_passed = Counter(
    'rfsn_tests_passed_total',
    'Number of passing tests',
    ['test_type'],
    registry=REGISTRY
)

tests_failed = Counter(
    'rfsn_tests_failed_total',
    'Number of failing tests',
    ['test_type'],
    registry=REGISTRY
)

test_duration = Histogram(
    'rfsn_test_duration_seconds',
    'Test execution time',
    buckets=[0.1, 0.5, 1, 2, 5, 10, 30, 60, 120, 300],
    registry=REGISTRY
)

# =============================================================================
# LLM Metrics
# =============================================================================

llm_requests = Counter(
    'rfsn_llm_requests_total',
    'Total LLM API requests',
    ['model', 'purpose'],  # purpose: 'planning', 'analysis', 'generation'
    registry=REGISTRY
)

llm_tokens = Counter(
    'rfsn_llm_tokens_total',
    'Total tokens consumed',
    ['model', 'token_type'],  # token_type: 'prompt', 'completion'
    registry=REGISTRY
)

llm_latency = Histogram(
    'rfsn_llm_latency_seconds',
    'LLM API response latency',
    buckets=[0.1, 0.5, 1, 2, 5, 10, 20, 30, 60],
    registry=REGISTRY
)

llm_errors = Counter(
    'rfsn_llm_errors_total',
    'LLM API errors',
    ['model', 'error_type'],
    registry=REGISTRY
)

# =============================================================================
# Cache Metrics
# =============================================================================

cache_hits = Counter(
    'rfsn_cache_hits_total',
    'Number of cache hits',
    ['cache_tier'],  # 'memory', 'disk', 'semantic'
    registry=REGISTRY
)

cache_misses = Counter(
    'rfsn_cache_misses_total',
    'Number of cache misses',
    ['cache_tier'],
    registry=REGISTRY
)

cache_size = Gauge(
    'rfsn_cache_size_bytes',
    'Current cache size in bytes',
    ['cache_tier'],
    registry=REGISTRY
)

cache_evictions = Counter(
    'rfsn_cache_evictions_total',
    'Number of cache evictions',
    ['cache_tier', 'reason'],  # reason: 'ttl', 'size', 'lru'
    registry=REGISTRY
)

# =============================================================================
# Gate Metrics
# =============================================================================

gate_validations = Counter(
    'rfsn_gate_validations_total',
    'Total gate validations',
    ['validation_type'],  # 'safety', 'schema', 'ordering', 'bounds'
    registry=REGISTRY
)

gate_rejections = Counter(
    'rfsn_gate_rejections_total',
    'Gate rejections by reason',
    ['rejection_reason'],
    registry=REGISTRY
)

gate_latency = Histogram(
    'rfsn_gate_latency_seconds',
    'Gate validation latency',
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1],
    registry=REGISTRY
)

# =============================================================================
# Docker/Sandbox Metrics
# =============================================================================

docker_operations = Counter(
    'rfsn_docker_operations_total',
    'Docker operations executed',
    ['operation'],  # 'create', 'start', 'stop', 'remove'
    registry=REGISTRY
)

docker_duration = Histogram(
    'rfsn_docker_operation_duration_seconds',
    'Docker operation duration',
    buckets=[0.1, 0.5, 1, 2, 5, 10, 30, 60],
    registry=REGISTRY
)

active_sandboxes = Gauge(
    'rfsn_active_sandboxes',
    'Number of active Docker sandboxes',
    registry=REGISTRY
)

# =============================================================================
# System Info
# =============================================================================

system_info = Info(
    'rfsn_system',
    'RFSN Controller system information',
    registry=REGISTRY
)

# =============================================================================
# Helper Functions
# =============================================================================

def start_metrics_server(port: int = 9090, addr: str = '0.0.0.0') -> None:
    """Start Prometheus metrics HTTP server.
    
    Args:
        port: Port to bind to (default: 9090)
        addr: Address to bind to (default: '0.0.0.0')
        
    Example:
        >>> start_metrics_server(port=9090)
        >>> # Metrics available at http://localhost:9090/metrics
    """
    start_http_server(port, addr=addr, registry=REGISTRY)
    print(f"Metrics server started on http://{addr}:{port}/metrics")


def record_repair_session(
    success: bool,
    duration: float,
    project_type: str = "unknown",
    failure_reason: str | None = None
) -> None:
    """Record repair session metrics.
    
    Args:
        success: Whether repair succeeded
        duration: Session duration in seconds
        project_type: Type of project (python, node, etc.)
        failure_reason: Reason for failure if unsuccessful
    """
    repair_duration.observe(duration)
    
    if success:
        repair_success.labels(project_type=project_type).inc()
    else:
        repair_failure.labels(
            failure_reason=failure_reason or "unknown",
            project_type=project_type
        ).inc()


def record_llm_call(
    model: str,
    purpose: str,
    duration: float,
    prompt_tokens: int,
    completion_tokens: int,
    error: str | None = None
) -> None:
    """Record LLM API call metrics.
    
    Args:
        model: LLM model name
        purpose: Purpose of call ('planning', 'analysis', 'generation')
        duration: Call duration in seconds
        prompt_tokens: Number of prompt tokens
        completion_tokens: Number of completion tokens
        error: Error type if call failed
    """
    llm_requests.labels(model=model, purpose=purpose).inc()
    llm_latency.observe(duration)
    
    if error:
        llm_errors.labels(model=model, error_type=error).inc()
    else:
        llm_tokens.labels(model=model, token_type='prompt').inc(prompt_tokens)
        llm_tokens.labels(model=model, token_type='completion').inc(completion_tokens)


def record_cache_access(
    tier: str,
    hit: bool,
    size_bytes: int | None = None
) -> None:
    """Record cache access metrics.
    
    Args:
        tier: Cache tier ('memory', 'disk', 'semantic')
        hit: Whether cache hit occurred
        size_bytes: Size of cached item if available
    """
    if hit:
        cache_hits.labels(cache_tier=tier).inc()
    else:
        cache_misses.labels(cache_tier=tier).inc()
    
    if size_bytes is not None:
        cache_size.labels(cache_tier=tier).set(size_bytes)


def initialize_system_info(version: str, python_version: str) -> None:
    """Initialize system information metric.
    
    Args:
        version: RFSN Controller version
        python_version: Python runtime version
    """
    system_info.info({
        'version': version,
        'python_version': python_version,
    })


# =============================================================================
# Context Manager Tracking (Convenience Wrappers)
# =============================================================================

from collections.abc import Generator
from contextlib import contextmanager


@contextmanager
def track_llm_call(provider: str, model: str) -> Generator[None, None, None]:
    """
    Context manager for tracking LLM calls with automatic timing.
    
    Args:
        provider: LLM provider name (e.g., 'deepseek', 'gemini')
        model: Model name (e.g., 'deepseek-chat', 'gemini-2.0-flash')
        
    Example:
        >>> with track_llm_call(provider="deepseek", model="deepseek-chat"):
        ...     response = llm.generate(prompt)
    """
    start_time = time.time()
    error = None
    try:
        yield
    except Exception as e:
        error = type(e).__name__
        raise
    finally:
        duration = time.time() - start_time
        llm_requests.labels(model=f"{provider}/{model}", purpose="general").inc()
        llm_latency.observe(duration)
        if error:
            llm_errors.labels(model=f"{provider}/{model}", error_type=error).inc()


@contextmanager
def track_cache_operation(tier: str, operation: str) -> Generator[None, None, None]:
    """
    Context manager for tracking cache operations.
    
    Args:
        tier: Cache tier ('memory', 'disk', 'semantic')
        operation: Operation type ('get', 'set', 'delete')
        
    Example:
        >>> with track_cache_operation(tier="memory", operation="get"):
        ...     value = cache.get(key)
    """
    try:
        yield
    except KeyError:
        # Cache miss
        cache_misses.labels(cache_tier=tier).inc()
        raise
    else:
        # Cache hit
        if operation == "get":
            cache_hits.labels(cache_tier=tier).inc()


@contextmanager
def track_patch_application(phase: str) -> Generator[None, None, None]:
    """
    Context manager for tracking patch applications.
    
    Args:
        phase: Repair phase ('verification', 'gate_validation', 'execution')
        
    Example:
        >>> with track_patch_application(phase="verification"):
        ...     result = apply_patch(patch)
    """
    start_time = time.time()
    success = False
    try:
        yield
        success = True
    finally:
        time.time() - start_time
        if success:
            patches_applied.labels(phase=phase).inc()
        else:
            patches_rejected.labels(reason="execution_failed", phase=phase).inc()


def get_metrics_text() -> str:
    """
    Get metrics in Prometheus text format.
    
    Returns:
        Metrics as text string
        
    Example:
        >>> metrics = get_metrics_text()
        >>> print(metrics)
    """
    from prometheus_client import generate_latest
    return generate_latest(REGISTRY).decode('utf-8')
